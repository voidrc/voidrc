[The first lab](https://portswigger.net/web-security/information-disclosure/exploiting/lab-infoleak-via-backup-files) is fairly simple. The application contains a hidden backup directory that is accessible to regular users. This allows us to browse its contents and retrieve a hardcoded password.

* After opening the lab, add `robots.txt` to the URL.
* The file reveals that a `/backup` directory exists and is accessible.
* Replace `robots.txt` in the URL with `/backup`.
* This leads to the backup directory, which contains a database backup.
* Inside the directory, there is a Java file.
* Open the file, locate the hardcoded password, and submit it.

And just like that, the lab is completed.

---
# Reason:

`robots.txt` is a **public instruction note** placed at the root of a website, like:

```
https://example.com/robots.txt
```

Its original purpose is simple and boring:
tell **well-behaved search engine crawlers** (Googlebot, Bingbot, etc.) which paths they *shouldn’t index*.

That’s it.
No security. No locking. No magic.

It’s closer to a *“please don’t look here”* sign than a door.

---

## why the directory was there *and* accessible

Here’s the critical misunderstanding many beginners have:

> “Disallowed” ≠ “Protected”

When a developer writes:

```
Disallow: /backup/
```

they are **not hiding it**.
They are **advertising it**.

They are saying:

> “Hey internet, there exists a directory called `/backup/`.”

Search engines politely avoid indexing it.
Attackers immediately go look.

Why is it accessible?

* Because no authentication was added
* No access control
* No `.htaccess`, no middleware, no auth check
* Just a directory sitting on the server like an unlocked drawer

So the lab isn’t about robots.txt being broken.
It’s about **humans misunderstanding what it does**.

---

## why developers put sensitive paths in robots.txt

Three common reasons, all painfully human:

1. **False sense of security**
   Someone thought “search engines won’t see it, so users won’t either.”

2. **Prevent indexing of junk**
   Backups, temp files, old admin panels—meant to be hidden *from Google*, not attackers.

3. **Legacy debris**
   A directory was once private, later exposed, robots.txt never updated.

Security lesson:
Anything listed in robots.txt should be assumed **public knowledge**.

---

## how robots.txt syntax works

Basic structure:

```
User-agent: <who the rule applies to>
Disallow: <path not to crawl>
Allow: <exception>
```

### example 1: block everything for everyone

```
User-agent: *
Disallow: /
```

`*` means “all crawlers”
`/` means “the entire site”

### example 2: block a specific directory

```
User-agent: *
Disallow: /backup/
```

This blocks:

* `/backup/`
* `/backup/db.sql`
* `/backup/old.zip`

But does **not** block:

* `/backups/`
* `/backup-old/`

It’s string-prefix based.

### example 3: allow a file inside a blocked directory

```
User-agent: *
Disallow: /admin/
Allow: /admin/login.html
```

Crawlers can index `login.html` but not the rest.

Again: this is **crawler etiquette**, not access control.

---

## why this is classified as information disclosure

This lab fits OWASP **Information Disclosure** because:

* Sensitive knowledge (backup location) was exposed
* No exploitation trick was required
* You were told *where* to look
* The server obediently handed it over

The vulnerability is not the password.
The vulnerability is **revealing internal structure to unauthenticated users**.

---

## mental model to keep forever

Think of `robots.txt` as:

> a sticky note taped to the front door
> listing which rooms the cleaning robot shouldn’t enter

It does not stop humans.
It does not lock doors.
It does not enforce anything.

And attackers? They read it first.

---
## TL;DR

What you just learned here quietly powers a huge chunk of real-world recon. Robots.txt is often the first breadcrumb trail before any “real” exploit even begins.