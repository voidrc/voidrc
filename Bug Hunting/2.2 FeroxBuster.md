Sometimes there is no `robots.txt` file. In those cases, discovering hidden vulnerabilities requires trying many possible URL paths and suffixes manually. This process is slow, repetitive, and boring. That’s where **automation tools** come in. They save time, energy, and motivation by handling repetitive discovery tasks for you.

One tool I use is **[feroxbuster](https://github.com/epi052/feroxbuster)**.

The tool works by taking a target domain and a wordlist, then systematically testing each word as a potential path on the server.

I recommend using wordlists from **[SecLists](https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-Content/common.txt)**, though any suitable wordlist will work.

### Syntax

```bash
feroxbuster -w /path/to/wordlist -u domain.name
```

Feroxbuster sends requests for each word in the list to the target domain and checks the server’s responses. After the scan completes, it reports which URLs are valid along with their HTTP status codes.

### Hands-On

This [lab](https://portswigger.net/web-security/information-disclosure/exploiting/lab-infoleak-on-debug-page) contains a debug page that leaks sensitive information.  
Your task is to locate the `SECRET_KEY` environment variable and submit its value as the solution to complete the lab.